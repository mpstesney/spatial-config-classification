{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monasteries:  19\n",
      "mosques : 20\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Vignette 2\n",
    "\n",
    "Join all buildings into single graph and create a single model\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# list paths for all monastery .JSONs\n",
    "mon_path = os.path.join(os.path.dirname(os.getcwd()), 'data\\monastaries\\jsons_features')\n",
    "\n",
    "mon_files = []\n",
    "for i in os.listdir(mon_path):\n",
    "    if i.endswith('.json'):\n",
    "        mon_files.append(os.path.join(mon_path, i))\n",
    "\n",
    "# list of paths for all mosque .JSONs\n",
    "mos_path = os.path.join(os.path.dirname(os.getcwd()), 'data\\mosques\\jsons_features')\n",
    "\n",
    "mos_files = []\n",
    "for i in os.listdir(mos_path):\n",
    "    if i.endswith('.json'):\n",
    "        mos_files.append(os.path.join(mos_path, i))                \n",
    "\n",
    "print(\"monasteries: \", len(mon_files))\n",
    "print(\"mosques :\", len(mos_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes:  1531\n",
      "edges:  1957\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Combine all jsons into single json in which all buildings are connected through a single ground node\n",
    "\"\"\"\n",
    "\n",
    "# empty dictionary\n",
    "ground = {'id': \"GR0000_000\", 'area': 0, 'iso_area': 0} # need to add features to this \n",
    "\n",
    "all_buildings_ground = {\n",
    "                 'directed': False,\n",
    "                 'multigraph': False,\n",
    "                 'graph': [],\n",
    "                 'nodes': [ground],\n",
    "                 'links': []\n",
    "                }\n",
    "\n",
    "# loop through all buildings to create single JSON\n",
    "files = mon_files + mos_files\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # delete 'exterior' node from building\n",
    "    ext_node = {}\n",
    "    for i, node in enumerate(data['nodes']):\n",
    "        if node['id'][-3:] == '000':\n",
    "            ext_node = node['id']\n",
    "            del data['nodes'][i]\n",
    "    \n",
    "    # swap edges that connect to exterior for edges that connect to new ground\n",
    "    for link in data['links']:\n",
    "        if link['source'] == ext_node:\n",
    "            link['source'] = ground['id']\n",
    "        elif link['target'] == ext_node:\n",
    "            link['target'] = ground['id']\n",
    "    \n",
    "    all_buildings_ground['graph'].append(data['graph'])\n",
    "    all_buildings_ground['nodes'].extend(data['nodes'])\n",
    "    all_buildings_ground['links'].extend(data['links'])   \n",
    "    \n",
    "# save JSON\n",
    "path = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\all_buildings_ground_features.json')\n",
    "\n",
    "with open(path, 'w', encoding ='utf8') as json_file: \n",
    "    json.dump(all_buildings_ground, json_file, indent=4)\n",
    "\n",
    "print('nodes: ', len(all_buildings_ground['nodes']))\n",
    "print('edges: ', len(all_buildings_ground['links']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|â–ˆ| 1531/1531 [00:00<00:00, 3458.42it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create node2vec model\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "# node2vec parameters\n",
    "# d  = 64   # dimensions\n",
    "# wl = 100   # walk_length\n",
    "# nw  = 100  # num_walks\n",
    "# w  = 4    # workers\n",
    "# p_val = 1.0\n",
    "# q_val = 2.0\n",
    "\n",
    "# node2vec parameters\n",
    "d  = 64   # dimensions\n",
    "wl = 100   # walk_length\n",
    "nw  = 100  # num_walks\n",
    "w  = 4    # workers\n",
    "p_val = 1.0\n",
    "q_val = 0.5\n",
    "\n",
    "# convert node_link JSON file to networkx graph and create a node2vec embeddings\n",
    "def embed(data, p_val, q_val, d, wl, nw, w):\n",
    "    # default = {\"source\": \"source\", \"target\": \"target\", \"name\": \"id\", \"key\": \"key\", \"link\": \"links\"}\n",
    "    default = {\"name\": \"id\"} # deals pydot bug but may not be necessary\n",
    "    G = json_graph.node_link_graph(data, attrs=default)\n",
    "    \n",
    "    # Precompute probabilities and generate walks\n",
    "    node2vec = Node2Vec(G, p=p_val, q=q_val, dimensions=d, walk_length=wl, num_walks=nw, workers=w)\n",
    "\n",
    "    ## if d_graph is big enough to fit in the memory, pass temp_folder which has enough disk space\n",
    "    # Note: It will trigger \"sharedmem\" in Parallel, which will be slow on smaller graphs\n",
    "    #node2vec = Node2Vec(graph, dimensions=64, walk_length=30, num_walks=200, workers=4, temp_folder=\"/mnt/tmp_data\")\n",
    "\n",
    "    # Embed\n",
    "    model = node2vec.fit(window=10, min_count=1, batch_words=4)  # Any keywords acceptable by gensim.Word2Vec can be passed, `diemnsions` and `workers` are automatically passed (from the Node2Vec constructor)\n",
    "\n",
    "    return model\n",
    "\n",
    "# add embeddings to building json\n",
    "def add_embed(data, model):\n",
    "    \n",
    "    model_vectors = model.wv.vectors\n",
    "    model_indices = {node: model.wv.vocab[node].index for node in model.wv.vocab}\n",
    "\n",
    "    # add embedding vector to each node and create single vector for whole building\n",
    "    nodes = []\n",
    "    bldgs = {}\n",
    "    for node in data['nodes']:\n",
    "        \n",
    "        # add vector to each node\n",
    "        i = model_indices[node['id']]\n",
    "        vector = [e.item() for e in model_vectors[i]]\n",
    "        node['vector'] = vector\n",
    "        nodes.append(model_vectors[i])\n",
    "        \n",
    "        # add vector to temp building list in temp dict\n",
    "        if node['id'] != \"GR0000_000\":\n",
    "            b = node['building_name']\n",
    "            if b not in list(bldgs.keys()):\n",
    "                bldgs[b] = [vector]\n",
    "            else:\n",
    "                bldgs[b].append(vector)\n",
    "\n",
    "    # calculate centroid of each building and add to building\n",
    "    for bldg in data['graph']:\n",
    "        nodes = np.array(bldgs[bldg['building_name']])\n",
    "        centroid_array = np.mean(nodes, axis=0)\n",
    "        centroid = [e.item() for e in centroid_array]\n",
    "        bldg['centroid'] = centroid\n",
    "\n",
    "    return data\n",
    "\n",
    "# create node2vec model\n",
    "model = embed(all_buildings_ground, p_val, q_val, d, wl, nw, w)\n",
    "print('model done')\n",
    "\n",
    "# update json with embeddings\n",
    "updated = add_embed(all_buildings_ground, model)\n",
    "\n",
    "# save new json with embeddings added\n",
    "# path = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\all_buildings_ground_embeddings.json')\n",
    "path = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\all_buildings_ground_embeddings2.json')\n",
    "\n",
    "with open(path, 'w', encoding ='utf8') as json_file: \n",
    "    json.dump(updated, json_file, indent=4) \n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Save building and node cvs files\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "\n",
    "# Update buildings csv\n",
    "buildings = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\all_buildings_features.csv')\n",
    "\n",
    "headers = []\n",
    "rows = []\n",
    "with open(buildings, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    headers = next(reader)\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "\n",
    "    # update each row of the building csv with the building centroid\n",
    "    i = headers.index('building_name')\n",
    "    comps = 0\n",
    "    for row in rows:\n",
    "        centroid = next(bldg['centroid'] for bldg in updated['graph'] if bldg[\"building_name\"] == row[i])\n",
    "        comps = len(centroid)\n",
    "        row.extend(centroid)\n",
    "    headers.extend(['c' + str(i) for i in range(comps)])\n",
    "\n",
    "# updated_buildings = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\all_buildings_ground_embeddings.csv')\n",
    "updated_buildings = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\all_buildings_ground_embeddings2.csv')\n",
    "\n",
    "with open(updated_buildings, 'w', newline=\"\", encoding='utf-8') as outFile: \n",
    "    writer = csv.writer(outFile)\n",
    "    writer.writerow(headers)\n",
    "    for row in rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "# # Update nodes csv    \n",
    "nodes = nodes = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\all_nodes_features.csv')\n",
    "\n",
    "headers = []\n",
    "rows = []\n",
    "new_rows = []\n",
    "with open(nodes, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    headers = next(reader)\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "    \n",
    "    # update each row of the node csv with node vector\n",
    "    i = headers.index('id')\n",
    "    comps = 0\n",
    "    for row in rows:\n",
    "        if not row[i].endswith('00'):\n",
    "            vector = next(node['vector'] for node in updated['nodes'] if node[\"id\"] == row[i])\n",
    "            comps = len(vector)\n",
    "            row.extend(vector)\n",
    "            new_row = row\n",
    "            new_rows.append(new_row)\n",
    "    headers.extend(['c' + str(i) for i in range(comps)])\n",
    "\n",
    "# updated_nodes = nodes = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\all_nodes_ground_embeddings.csv')\n",
    "updated_nodes = nodes = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\all_nodes_ground_embeddings2.csv')\n",
    "\n",
    "with open(updated_nodes, 'w', newline=\"\", encoding='utf-8') as outFile: \n",
    "    writer = csv.writer(outFile)\n",
    "    writer.writerow(headers)\n",
    "    for row in new_rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
