{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monasteries:  19\n",
      "mosques : 20\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Vignette 2\n",
    "\n",
    "Add node and graph attributes to each building json file\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "\n",
    "# list paths for all monastery .JSONs\n",
    "mon_path = os.path.join(os.path.dirname(os.getcwd()), 'data\\monastaries\\jsons_named')\n",
    "mon_files = []\n",
    "for i in os.listdir(mon_path):\n",
    "    if i.endswith('.json'):\n",
    "        mon_files.append(os.path.join(mon_path, i))\n",
    "\n",
    "# list of paths for all mosque .JSONs\n",
    "not_used = ['MS1550.json', 'MS1562.json']\n",
    "mos_path = os.path.join(os.path.dirname(os.getcwd()), 'data\\mosques\\jsons_named')\n",
    "mos_files = []\n",
    "for i in os.listdir(mos_path):\n",
    "    if i.endswith('.json') and i not in not_used:\n",
    "        mos_files.append(os.path.join(mos_path, i))                \n",
    "\n",
    "print(\"monasteries: \", len(mon_files))\n",
    "print(\"mosques :\", len(mos_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "open each json\n",
    "convert to networkX graph\n",
    "\n",
    "analyze each node\n",
    "add analysis results as node attributes\n",
    "\n",
    "analyze graph\n",
    "add analysis results as graph attributes\n",
    "\n",
    "NetworkX algorithms nodes:\n",
    "- betweenness centrality\n",
    "https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.\n",
    "algorithms.centrality.betweenness_centrality.html#networkx.algorithms.centrality.betweenness_centrality\n",
    "\n",
    "- degree centrality\n",
    "https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.\n",
    "algorithms.centrality.degree_centrality.html#networkx.algorithms.centrality.degree_centrality\n",
    "\n",
    "- eignenvector centrality\n",
    "https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.\n",
    "algorithms.centrality.eigenvector_centrality.html#networkx.algorithms.centrality.eigenvector_centrality\n",
    "\n",
    "- closeness centrality\n",
    "https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.\n",
    "algorithms.centrality.closeness_centrality.html#networkx.algorithms.centrality.closeness_centrality\n",
    "\n",
    "- local clustering coefficient\n",
    "https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.\n",
    "algorithms.cluster.clustering.html\n",
    "\n",
    "NetworkX algorithms graph\n",
    "- average local clustering coefficient\n",
    "https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.\n",
    "algorithms.cluster.average_clustering.html\n",
    "\n",
    "- global clustering\n",
    "https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.\n",
    "algorithms.cluster.transitivity.html#networkx.algorithms.cluster.transitivity\n",
    "\n",
    "- clique analysis\n",
    "https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.\n",
    "algorithms.clique.find_cliques.html#networkx.algorithms.clique.find_cliques\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def add_node_attributes(bldg, bldg_type, bldg_name):\n",
    "    \"\"\" Add betweenness centrality, degree centrality, eigenvector centrality, closeness centrality\n",
    "     and local clustering coefficient to each node as an attribute\"\"\"\n",
    "    \n",
    "    nx.set_node_attributes(bldg, bldg_type, \"building_type\")\n",
    "    nx.set_node_attributes(bldg, bldg_name, \"building_name\")\n",
    "    \n",
    "    bc = nx.betweenness_centrality(bldg)\n",
    "    nx.set_node_attributes(bldg, bc, \"betweenness\")\n",
    "    \n",
    "    dc = nx.degree_centrality(bldg)\n",
    "    nx.set_node_attributes(bldg, dc, \"degree_centrality\")\n",
    "    \n",
    "    ec = nx.eigenvector_centrality(bldg)\n",
    "    nx.set_node_attributes(bldg, ec, \"eigenvector_centrality\")\n",
    "    \n",
    "    cc = nx.closeness_centrality(bldg)\n",
    "    nx.set_node_attributes(bldg, cc, \"closeness_centrality\")\n",
    "    \n",
    "    for node in bldg.nodes:\n",
    "        bldg.add_node(node, degree=nx.degree(bldg, node))\n",
    "        \n",
    "    clc = nx.clustering(bldg)\n",
    "    nx.set_node_attributes(bldg, clc, \"clustering_coef\")\n",
    "\n",
    "def add_graph_attributes(bldg, bldg_type, bldg_name):\n",
    "    \"\"\" Add number of nodes, density, average cluster coefficient and transistivity\n",
    "    to graph as attributes\"\"\"\n",
    "\n",
    "    bldg.graph['building_type'] = bldg_type\n",
    "    bldg.graph['building_name'] = bldg_name\n",
    "    bldg.graph['num_nodes'] = nx.number_of_nodes(bldg)\n",
    "    bldg.graph['num_edges'] = nx.number_of_edges(bldg)\n",
    "    bldg.graph['density'] = nx.density(bldg)\n",
    "    bldg.graph['ave_cluster_coef'] = nx.average_clustering(bldg)\n",
    "    # global clustering coefficient\n",
    "    bldg.graph['transistivity'] = nx.transitivity(bldg)\n",
    "    # create list of cliques greater than 2\n",
    "    cliques = [clique for clique in list(nx.find_cliques(bldg)) if len(clique) > 2]\n",
    "    bldg.graph['cliques'] = cliques    \n",
    "\n",
    "\n",
    "# loop through all buildings to create single JSON\n",
    "files = mon_files + mos_files\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "        # determine building type from file name\n",
    "        i = file.rfind('\\\\') + 1\n",
    "        filename = file[i:]\n",
    "        bldg_name = filename[:6]\n",
    "        if (filename.startswith('MN')):\n",
    "            bldg_type = 'monastery'\n",
    "        else:\n",
    "            bldg_type = 'mosque'\n",
    "    \n",
    "    # construct networkx graph and add node and graph attributes\n",
    "    G = json_graph.node_link_graph(data, {\"name\": \"id\"})\n",
    "    add_node_attributes(G, bldg_type, bldg_name)\n",
    "    add_graph_attributes(G, bldg_type, bldg_name)\n",
    "    \n",
    "    # save graph as new json file\n",
    "    if bldg_type == 'mosque':\n",
    "        path = os.path.join(os.path.dirname(os.getcwd()), 'data\\mosques\\jsons_features')\n",
    "    else:\n",
    "        path = os.path.join(os.path.dirname(os.getcwd()), 'data\\monastaries\\jsons_features')\n",
    "\n",
    "    out_path = os.path.join(path, filename)\n",
    "    out = json_graph.node_link_data(G, {\"name\": \"id\"})\n",
    "    with open(out_path, 'w', encoding ='utf8') as json_file: \n",
    "        json.dump(out, json_file, indent=4)\n",
    "#         print(f'saving: {filename}')\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monasteries:  19\n",
      "mosques : 20\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create CSV file for building instances\n",
    "Create CVS file for node instances\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "\n",
    "# list paths for all monastery .JSONs\n",
    "mon_path = os.path.join(os.path.dirname(os.getcwd()), 'data\\monastaries\\jsons_features')\n",
    "mon_files = []\n",
    "for i in os.listdir(mon_path):\n",
    "    if i.endswith('.json'):\n",
    "        mon_files.append(os.path.join(mon_path, i))\n",
    "\n",
    "# list of paths for all mosque .JSONs\n",
    "not_used = ['MS1550.json', 'MS1562.json']\n",
    "mos_path = os.path.join(os.path.dirname(os.getcwd()), 'data\\mosques\\jsons_features')\n",
    "mos_files = []\n",
    "for i in os.listdir(mos_path):\n",
    "    if i.endswith('.json') and i not in not_used:\n",
    "        mos_files.append(os.path.join(mos_path, i))                \n",
    "\n",
    "print(\"monasteries: \", len(mon_files))\n",
    "print(\"mosques :\", len(mos_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Open each json file and save data to buildings and nodes csv files\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "buildings = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\buildings.csv')\n",
    "bldgs_header = []\n",
    "bldgs_rows = []\n",
    "\n",
    "nodes = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\nodes.csv')\n",
    "nodes_header = []\n",
    "nodes_rows = []\n",
    "\n",
    "# Open each json file. Add each building and each node to individual row for csvs\n",
    "files = mon_files + mos_files\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "        # buildings rows\n",
    "        if bldgs_header == []:\n",
    "            bldgs_header = list(data['graph'].keys())\n",
    "        bldgs_rows.append(list(data['graph'].values())) \n",
    "           \n",
    "        # node rows    \n",
    "        if nodes_header == []:\n",
    "            nodes_header = list(data['nodes'][0].keys())\n",
    "        for node in data['nodes']:\n",
    "            nodes_rows.append(list(node.values()))\n",
    "\n",
    "# Add normalized columns of building num_nodes and num_edges\n",
    "i = bldgs_header.index('num_nodes')\n",
    "j = bldgs_header.index('num_edges')\n",
    "max_nodes = max([row[i] for row in bldgs_rows])\n",
    "max_edges = max([row[j] for row in bldgs_rows])\n",
    "bldgs_header.extend(['num_nodes_norm', 'num_edges_norm'])\n",
    "for row in bldgs_rows:\n",
    "    row.extend([row[i] / max_nodes, row[j] / max_edges])\n",
    "\n",
    "# Add normalized columns of node area, iso_area and degree\n",
    "i = nodes_header.index('area')\n",
    "j = nodes_header.index('iso_area')\n",
    "k = nodes_header.index('degree')\n",
    "max_area = max([row[i] for row in nodes_rows])\n",
    "max_iso_area = max([row[j] for row in nodes_rows])\n",
    "max_degree = max([row[k] for row in nodes_rows])\n",
    "nodes_header.extend(['area_norm', 'iso_area_norm', 'degree_norm'])\n",
    "for row in nodes_rows:\n",
    "    row.extend([row[i] / max_area, row[j] / max_iso_area, row[k] / max_degree])\n",
    "\n",
    "# Write buildings csv\n",
    "with open(buildings, 'w', newline=\"\", encoding='utf-8') as outFile: \n",
    "    writer = csv.writer(outFile)\n",
    "    writer.writerow(bldgs_header)\n",
    "    for row in bldgs_rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "# Write nodes csv\n",
    "with open(nodes, 'w', newline=\"\", encoding='utf-8') as outFile: \n",
    "    writer = csv.writer(outFile)\n",
    "    writer.writerow(nodes_header)\n",
    "    for row in nodes_rows:\n",
    "        writer.writerow(row)\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
