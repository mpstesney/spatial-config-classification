{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monasteries:  19\n",
      "mosques : 20\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Vignette 2\n",
    "\n",
    "Import all .JSON files for mosques and monastaries\n",
    "Creating noded embeddings using node2vec\n",
    "Save embeddings and models to files\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# list paths for all monastery .JSONs\n",
    "mon_path = os.path.join(os.path.dirname(os.getcwd()), 'data\\monastaries\\jsons_features')\n",
    "mon_files = []\n",
    "for i in os.listdir(mon_path):\n",
    "    if i.endswith('.json'):\n",
    "        mon_files.append(os.path.join(mon_path, i))\n",
    "\n",
    "# list of paths for all mosque .JSONs\n",
    "# mos_path = r'G:\\My Drive\\Classes\\Thesis\\project\\ferrando_data\\my_work\\mosques\\jsons'\n",
    "mos_path = os.path.join(os.path.dirname(os.getcwd()), 'data\\mosques\\jsons_features')\n",
    "mos_files = []\n",
    "for i in os.listdir(mos_path):\n",
    "    if i.endswith('.json'):\n",
    "        mos_files.append(os.path.join(mos_path, i))                \n",
    "\n",
    "print(\"monasteries: \", len(mon_files))\n",
    "print(\"mosques :\", len(mos_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|████| 25/25 [00:00<00:00, 2275.70it/s]\n",
      "Computing transition probabilities: 100%|████| 30/30 [00:00<00:00, 3345.81it/s]\n",
      "Computing transition probabilities: 100%|████| 33/33 [00:00<00:00, 5533.60it/s]\n",
      "Computing transition probabilities: 100%|████| 38/38 [00:00<00:00, 5430.63it/s]\n",
      "Computing transition probabilities: 100%|████| 30/30 [00:00<00:00, 4992.82it/s]\n",
      "Computing transition probabilities: 100%|████| 25/25 [00:00<00:00, 5014.47it/s]\n",
      "Computing transition probabilities: 100%|████| 23/23 [00:00<00:00, 3805.33it/s]\n",
      "Computing transition probabilities: 100%|████| 37/37 [00:00<00:00, 6238.26it/s]\n",
      "Computing transition probabilities: 100%|████| 31/31 [00:00<00:00, 2590.37it/s]\n",
      "Computing transition probabilities: 100%|████| 35/35 [00:00<00:00, 5013.17it/s]\n",
      "Computing transition probabilities: 100%|████| 42/42 [00:00<00:00, 2339.64it/s]\n",
      "Computing transition probabilities: 100%|████| 74/74 [00:00<00:00, 6773.27it/s]\n",
      "Computing transition probabilities: 100%|████| 39/39 [00:00<00:00, 3920.66it/s]\n",
      "Computing transition probabilities: 100%|████| 15/15 [00:00<00:00, 3033.34it/s]\n",
      "Computing transition probabilities: 100%|████| 59/59 [00:00<00:00, 3944.65it/s]\n",
      "Computing transition probabilities: 100%|████| 37/37 [00:00<00:00, 4637.50it/s]\n",
      "Computing transition probabilities: 100%|████| 69/69 [00:00<00:00, 6917.49it/s]\n",
      "Computing transition probabilities: 100%|████| 50/50 [00:00<00:00, 5011.36it/s]\n",
      "Computing transition probabilities: 100%|████| 35/35 [00:00<00:00, 3898.16it/s]\n",
      "Computing transition probabilities: 100%|████| 95/95 [00:00<00:00, 3660.25it/s]\n",
      "Computing transition probabilities: 100%|████| 43/43 [00:00<00:00, 4291.31it/s]\n",
      "Computing transition probabilities: 100%|████| 10/10 [00:00<00:00, 3341.54it/s]\n",
      "Computing transition probabilities: 100%|████| 14/14 [00:00<00:00, 3507.99it/s]\n",
      "Computing transition probabilities: 100%|████| 75/75 [00:00<00:00, 1157.30it/s]\n",
      "Computing transition probabilities: 100%|████| 89/89 [00:00<00:00, 3076.73it/s]\n",
      "Computing transition probabilities: 100%|██| 130/130 [00:00<00:00, 3945.30it/s]\n",
      "Computing transition probabilities: 100%|████| 34/34 [00:00<00:00, 4918.31it/s]\n",
      "Computing transition probabilities: 100%|██████| 9/9 [00:00<00:00, 4504.62it/s]\n",
      "Computing transition probabilities: 100%|████| 46/46 [00:00<00:00, 4203.99it/s]\n",
      "Computing transition probabilities: 100%|██████| 4/4 [00:00<00:00, 4144.57it/s]\n",
      "Computing transition probabilities: 100%|████| 36/36 [00:00<00:00, 4512.16it/s]\n",
      "Computing transition probabilities: 100%|██████| 4/4 [00:00<00:00, 1981.25it/s]\n",
      "Computing transition probabilities: 100%|████| 74/74 [00:00<00:00, 5689.28it/s]\n",
      "Computing transition probabilities: 100%|████| 99/99 [00:00<00:00, 3817.49it/s]\n",
      "Computing transition probabilities: 100%|████| 14/14 [00:00<00:00, 7001.34it/s]\n",
      "Computing transition probabilities: 100%|████| 22/22 [00:00<00:00, 4391.11it/s]\n",
      "Computing transition probabilities: 100%|████| 11/11 [00:00<00:00, 5605.31it/s]\n",
      "Computing transition probabilities: 100%|████| 14/14 [00:00<00:00, 7109.85it/s]\n",
      "Computing transition probabilities: 100%|████| 19/19 [00:00<00:00, 9526.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loop through both lists of files and use node to vect to embed the nodes\n",
    "\n",
    "# https://github.com/eliorc/node2vec/blob/master/example.py\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "import numpy as np\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "# node2vec parameters - discover nodes with similar structural roles \n",
    "d  = 64   # dimensions\n",
    "wl = 100   # walk_length\n",
    "nw  = 200  # num_walks\n",
    "w  = 4    # workers\n",
    "p_val = 1.0\n",
    "q_val = 2.0\n",
    "\n",
    "# node2vec parameters - discovers homophily - clusters of nodes that frequently interact with each other\n",
    "# d  = 64   # dimensions\n",
    "# wl = 100   # walk_length\n",
    "# nw  = 200  # num_walks\n",
    "# w  = 4    # workers\n",
    "# p_val = 1.0\n",
    "# q_val = 0.5\n",
    "\n",
    "\n",
    "# convert node_link JSON file to networkx graph and create a node2vec embeddings\n",
    "def embed(data, p_val, q_val, d, wl, nw, w):\n",
    "    # default = {\"source\": \"source\", \"target\": \"target\", \"name\": \"id\", \"key\": \"key\", \"link\": \"links\"}\n",
    "    default = {\"name\": \"id\"} # deals pydot bug but may not be necessary\n",
    "    G = json_graph.node_link_graph(data, attrs=default)\n",
    "    \n",
    "    # Precompute probabilities and generate walks\n",
    "    node2vec = Node2Vec(G, p=p_val, q=q_val, dimensions=d, walk_length=wl, num_walks=nw, workers=w)\n",
    "\n",
    "    ## if d_graph is big enough to fit in the memory, pass temp_folder which has enough disk space\n",
    "    # Note: It will trigger \"sharedmem\" in Parallel, which will be slow on smaller graphs\n",
    "    #node2vec = Node2Vec(graph, dimensions=64, walk_length=30, num_walks=200, workers=4, temp_folder=\"/mnt/tmp_data\")\n",
    "\n",
    "    # Embed\n",
    "    model = node2vec.fit(window=10, min_count=1, batch_words=4)  # Any keywords acceptable by gensim.Word2Vec can be passed, `diemnsions` and `workers` are automatically passed (from the Node2Vec constructor)\n",
    "\n",
    "    return model\n",
    "\n",
    "# add embeddings to building json\n",
    "def add_embed(data, model):\n",
    "    \n",
    "    model_vectors = model.wv.vectors\n",
    "    model_indices = {node: model.wv.vocab[node].index for node in model.wv.vocab}\n",
    "\n",
    "    # add embedding vector to each node and create single vector for whole building\n",
    "    nodes = []\n",
    "    for node in data['nodes']:\n",
    "        i = model_indices[node['id']]\n",
    "        vector = [e.item() for e in model_vectors[i]]\n",
    "        node['vector'] = vector\n",
    "        nodes.append(model_vectors[i])\n",
    "    nodes = np.array(nodes)\n",
    "    centroid_array = np.mean(nodes, axis=0)\n",
    "    centroid = [e.item() for e in centroid_array]\n",
    "    data['graph']['centroid'] = centroid\n",
    "    \n",
    "    return data\n",
    "    \n",
    "# loop through all monastery JSONs and save embeddings\n",
    "mon_embed_path = os.path.join(os.path.dirname(os.getcwd()), 'data\\monastaries\\jsons_embeddings')\n",
    "# mon_embed_path = os.path.join(os.path.dirname(os.getcwd()), 'data\\monastaries\\jsons_embeddings2')\n",
    "\n",
    "for file in mon_files:\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # create node2vec model\n",
    "    model = embed(data, p_val, q_val, d, wl, nw, w)\n",
    "    \n",
    "    # update json with embeddings\n",
    "    updated = add_embed(data, model)\n",
    "    \n",
    "    # save files\n",
    "    i = file.rfind('\\\\')\n",
    "    monastery = file[i + 1:-4]\n",
    "    \n",
    "#     embedding_filename = os.path.join(mon_embed_path, monastery + 'emb')\n",
    "#     model.wv.save_word2vec_format(embedding_filename)\n",
    "    \n",
    "#     model_filename = os.path.join(mon_embed_path, monastery + 'model')\n",
    "#     model.save(model_filename)\n",
    "\n",
    "    json_filename = os.path.join(mon_embed_path, monastery + 'json')\n",
    "    with open(json_filename, 'w', encoding ='utf8') as json_file: \n",
    "        json.dump(updated, json_file, indent=4)   \n",
    "\n",
    "# loop through all mosque JSONs and save embeddings\n",
    "mos_embed_path = os.path.join(os.path.dirname(os.getcwd()), 'data\\mosques\\jsons_embeddings')\n",
    "# mos_embed_path = os.path.join(os.path.dirname(os.getcwd()), 'data\\mosques\\jsons_embeddings2')\n",
    "\n",
    "for file in mos_files:\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # create node2vec model\n",
    "    model = embed(data, p_val, q_val, d, wl, nw, w)\n",
    "    \n",
    "    # update json with embeddings\n",
    "    updated = add_embed(data, model)\n",
    "\n",
    "    # save files\n",
    "    i = file.rfind('\\\\')\n",
    "    mosque = file[i + 1:-4]\n",
    "    \n",
    "#     embedding_filename = os.path.join(mos_embed_path, mosque + 'emb')\n",
    "#     model.wv.save_word2vec_format(embedding_filename)\n",
    "    \n",
    "#     model_filename = os.path.join(mos_embed_path, mosque + 'model')\n",
    "#     model.save(model_filename)\n",
    "\n",
    "    json_filename = os.path.join(mos_embed_path, mosque + 'json')\n",
    "    with open(json_filename, 'w', encoding ='utf8') as json_file: \n",
    "        json.dump(updated, json_file, indent=4) \n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Add embeddings to CSV files\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Return vector of building centroid\n",
    "def get_centroid(bldg):\n",
    "    if bldg.startswith('MN'):\n",
    "        path = os.path.join(os.path.dirname(os.getcwd()), 'data\\monastaries\\jsons_embeddings')\n",
    "#         path = os.path.join(os.path.dirname(os.getcwd()), 'data\\monastaries\\jsons_embeddings2')\n",
    "    else:\n",
    "        path = os.path.join(os.path.dirname(os.getcwd()), 'data\\mosques\\jsons_embeddings')\n",
    "#         path = os.path.join(os.path.dirname(os.getcwd()), 'data\\mosques\\jsons_embeddings2')\n",
    "    file = os.path.join(path, bldg + '.json')\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    return data['graph']['centroid']\n",
    "\n",
    "# Update buildings csv\n",
    "buildings = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\buildings.csv')\n",
    "headers = []\n",
    "rows = []\n",
    "with open(buildings, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    headers = next(reader)\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "\n",
    "    # update each row of the building csv with the building centroid\n",
    "    i = headers.index('building_name')\n",
    "    comps = 0\n",
    "    for row in rows:\n",
    "        centroid = get_centroid(row[i])\n",
    "        comps = len(centroid)\n",
    "        row.extend(centroid)\n",
    "    headers.extend(['c' + str(i) for i in range(comps)])\n",
    "\n",
    "updated_buildings = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\buildings_embeddings.csv')    \n",
    "# updated_buildings = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\buildings_embeddings2.csv')\n",
    "with open(updated_buildings, 'w', newline=\"\", encoding='utf-8') as outFile: \n",
    "    writer = csv.writer(outFile)\n",
    "    writer.writerow(headers)\n",
    "    for row in rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "def get_nodes(bldg):\n",
    "    if bldg.startswith('MN'):\n",
    "        path = os.path.join(os.path.dirname(os.getcwd()), 'data\\monastaries\\jsons_embeddings')\n",
    "#         path = os.path.join(os.path.dirname(os.getcwd()), 'data\\monastaries\\jsons_embeddings2')\n",
    "    else:\n",
    "        path = os.path.join(os.path.dirname(os.getcwd()), 'data\\mosques\\jsons_embeddings')\n",
    "#         path = os.path.join(os.path.dirname(os.getcwd()), 'data\\mosques\\jsons_embeddings2')\n",
    "    file = os.path.join(path, bldg + '.json')\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    return data['nodes']\n",
    "\n",
    "# Update nodes csv    \n",
    "nodes = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\nodes.csv')\n",
    "headers = []\n",
    "rows = []\n",
    "with open(nodes, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    headers = next(reader)\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "    \n",
    "    # update each row of the node csv with node vector\n",
    "    i = headers.index('building_name')\n",
    "    j = headers.index('id')\n",
    "    comps = 0\n",
    "    prev_row = None\n",
    "    for row in rows:\n",
    "        if row[i] != prev_row: # what is this doing?\n",
    "            json_nodes = get_nodes(row[i])\n",
    "        vector = next(node['vector'] for node in json_nodes if node[\"id\"] == row[j])\n",
    "        comps = len(vector)\n",
    "        row.extend(vector)\n",
    "    headers.extend(['c' + str(i) for i in range(comps)])\n",
    "\n",
    "updated_nodes = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\nodes_embeddings.csv')\n",
    "# updated_nodes = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\nodes_embeddings2.csv')\n",
    "with open(updated_nodes, 'w', newline=\"\", encoding='utf-8') as outFile: \n",
    "    writer = csv.writer(outFile)\n",
    "    writer.writerow(headers)\n",
    "    for row in rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
