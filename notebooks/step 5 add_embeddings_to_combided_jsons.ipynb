{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vignette 2\n",
    "\n",
    "Import all .JSON file containing all building info and node information\n",
    "Create node embeddings using node2vec\n",
    "\n",
    "Save embeddings and models to files\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "import numpy as np\n",
    "from node2vec import Node2Vec\n",
    "import csv\n",
    "\n",
    "path = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\all_buildings_features.json')\n",
    "\n",
    "with open(path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loop through both lists of files and use node2vec to embed the nodes\n",
    "\n",
    "# https://github.com/eliorc/node2vec/blob/master/example.py\n",
    "\"\"\"\n",
    "\n",
    "# node2vec parameters - discover nodes with similar structural roles \n",
    "d  = 64   # dimensions 64\n",
    "wl = 100   # walk_length 100\n",
    "nw  = 200  # num_walks 200\n",
    "w  = 4    # workers\n",
    "p_val = 1.0\n",
    "q_val = 2.0\n",
    "\n",
    "# node2vec parameters - discovers homophily - clusters of nodes that frequently interact with each other\n",
    "# d  = 64   # dimensions\n",
    "# wl = 100   # walk_length\n",
    "# nw  = 200  # num_walks\n",
    "# w  = 4    # workers\n",
    "# p_val = 1.0\n",
    "# q_val = 0.5\n",
    "\n",
    "# convert node_link JSON file to networkx graph and create a node2vec embeddings\n",
    "def embed(data, p_val, q_val, d, wl, nw, w):\n",
    "    # default = {\"source\": \"source\", \"target\": \"target\", \"name\": \"id\", \"key\": \"key\", \"link\": \"links\"}\n",
    "    default = {\"name\": \"id\"} # deals pydot bug but may not be necessary\n",
    "    G = json_graph.node_link_graph(data, attrs=default)\n",
    "    \n",
    "    # Precompute probabilities and generate walks\n",
    "    node2vec = Node2Vec(G, p=p_val, q=q_val, dimensions=d, walk_length=wl, num_walks=nw, workers=w)\n",
    "\n",
    "    ## if d_graph is big enough to fit in the memory, pass temp_folder which has enough disk space\n",
    "    # Note: It will trigger \"sharedmem\" in Parallel, which will be slow on smaller graphs\n",
    "    #node2vec = Node2Vec(graph, dimensions=64, walk_length=30, num_walks=200, workers=4, temp_folder=\"/mnt/tmp_data\")\n",
    "\n",
    "    # Embed\n",
    "    model = node2vec.fit(window=10, min_count=1, batch_words=4)  # Any keywords acceptable by gensim.Word2Vec can be passed, `diemnsions` and `workers` are automatically passed (from the Node2Vec constructor)\n",
    "\n",
    "    return model\n",
    "\n",
    "# add embeddings to building json\n",
    "def add_embed(data, model):\n",
    "    \n",
    "    model_vectors = model.wv.vectors\n",
    "    model_indices = {node: model.wv.vocab[node].index for node in model.wv.vocab}\n",
    "\n",
    "    # add embedding vector to each node and create single vector for whole building\n",
    "    nodes = []\n",
    "    bldgs = {}\n",
    "    for node in data['nodes']:\n",
    "        \n",
    "        # add vector to each node\n",
    "        i = model_indices[node['id']]\n",
    "        vector = [e.item() for e in model_vectors[i]]\n",
    "        node['vector'] = vector\n",
    "        nodes.append(model_vectors[i])\n",
    "        \n",
    "        # add vector to temp building list in temp dict\n",
    "        b = node['building_name']\n",
    "        if b not in list(bldgs.keys()):\n",
    "            bldgs[b] = [vector]\n",
    "        else:\n",
    "            bldgs[b].append(vector)\n",
    "    \n",
    "    # calculate centroid of each building and add to building\n",
    "    for bldg in data['graph']:\n",
    "        nodes = np.array(bldgs[bldg['building_name']])\n",
    "        centroid_array = np.mean(nodes, axis=0)\n",
    "        centroid = [e.item() for e in centroid_array]\n",
    "        bldg['centroid'] = centroid\n",
    "\n",
    "    return data\n",
    "\n",
    "create node2vec model\n",
    "model = embed(data, p_val, q_val, d, wl, nw, w)\n",
    "\n",
    "update json with embeddings\n",
    "updated = add_embed(data, model)\n",
    "\n",
    "save new json with embeddings added\n",
    "# path = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\all_buildings_embeddings.json')\n",
    "path = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\all_buildings_embeddings2.json')\n",
    "\n",
    "with open(path, 'w', encoding ='utf8') as json_file: \n",
    "    json.dump(updated, json_file, indent=4) \n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add embeddings to CSV files\n",
    "\"\"\"\n",
    "\n",
    "# Update buildings csv\n",
    "buildings = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\all_buildings_features.csv')\n",
    "\n",
    "headers = []\n",
    "rows = []\n",
    "with open(buildings, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    headers = next(reader)\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "\n",
    "    # update each row of the building csv with the building centroid\n",
    "    i = headers.index('building_name')\n",
    "    comps = 0\n",
    "    for row in rows:\n",
    "        centroid = next(bldg['centroid'] for bldg in updated['graph'] if bldg[\"building_name\"] == row[i])\n",
    "        comps = len(centroid)\n",
    "        row.extend(centroid)\n",
    "    headers.extend(['c' + str(i) for i in range(comps)])\n",
    "\n",
    "# updated_buildings = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\all_buildings_embeddings.csv')\n",
    "updated_buildings = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\all_buildings_embeddings2.csv')\n",
    "with open(updated_buildings, 'w', newline=\"\", encoding='utf-8') as outFile: \n",
    "    writer = csv.writer(outFile)\n",
    "    writer.writerow(headers)\n",
    "    for row in rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "# Update nodes csv    \n",
    "nodes = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\all_nodes_features.csv')\n",
    "\n",
    "headers = []\n",
    "rows = []\n",
    "with open(nodes, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    headers = next(reader)\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "    \n",
    "    # update each row of the node csv with node vector\n",
    "    i = headers.index('id')\n",
    "    comps = 0\n",
    "    prev_row = None\n",
    "    for row in rows:\n",
    "        vector = next(node['vector'] for node in updated['nodes'] if node[\"id\"] == row[i])\n",
    "        comps = len(vector)\n",
    "        row.extend(vector)\n",
    "    headers.extend(['c' + str(i) for i in range(comps)])\n",
    "\n",
    "# updated_nodes = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\all_nodes_embeddings.csv')\n",
    "updated_nodes = os.path.join(os.path.dirname(os.getcwd()), 'data\\all\\all_nodes_embeddings2.csv')\n",
    "with open(updated_nodes, 'w', newline=\"\", encoding='utf-8') as outFile: \n",
    "    writer = csv.writer(outFile)\n",
    "    writer.writerow(headers)\n",
    "    for row in rows:\n",
    "        writer.writerow(row)        \n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
